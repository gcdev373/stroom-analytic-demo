# Analytic Demonstrator
## Overview
This analytic demonstrator comprise the following components:
1. **EventGen** - a self-contained stochastic event generator that creates text files
1. **Stroom** - Modified version of Stroom that provides enhanced support for Kafka
1. **Stroom content** - Feeds, XSLT, XSD and other configuration required to configure Stroom to accept EventGen data
and write it to a Kafka topic.
1. **Analytic content** - Kafka analytics, python code, etc.

In addition, the following components are also required:
1. Kafka - typically deployed to the same host as Stroom via docker.
1. Zookeeper - required by Kafka.
1. Supporting Stroom components - these are typically deployed to the same host as Stroom via docker.

## EventGen
This is a Java application that creates events in text files.  It is not necessary to process these files using Stroom, so 
CSV format is used, making it easy to process with any technology.

The events generated are determined by a stochastic (random) process to drive a finite state machine, 
all of which is controlled via a `yaml` format config file.
This configuration file also controls all other aspects of the `EventGen` application, including the set of files created.  

Usage: `java -jar event-gen-all.jar <path to config yaml>`

The demonstrator includes a configuration file called `ueba.yml`.  Running `EventGen` with this configuration results in
the creation of files in the default location (`/tmp/eventgen`) The following files are produced:
* DEMO-MAINFRAME-EVENTS (43589 lines)
* DEMO-VPN-EVENTS (10892 lines)
* special.out (10 lines)

The first two files contain event streams that are intended to be representative of those that could be created by 
real-world event (log) sources.  Therefore, it is these that are expected to be analysed by an analytic process.

The last file (`special.out`) is far smaller than the other files, it records events that are the result of rare
state transitions, and it is expected that these may be what an analytic process might identify.  In effect, this file
contains the "answers", to a number of as yet unposed questions.

#### Uploading to Stroom
It is necessary to upload the resultant `.txt` files to Stroom for processing.  The HTTP header `Feed` should be set correctly.
The example configuration file `ueba.yml` creates files that have a filename that corresponds to the appropriate Stroom feed.

A script `demonstrator/bash/sendToStroom.sh` is provided to achieve this.  It should be run from the directory containing
the output from `EventGen`, for example:
```Shell
cd /tmp/eventgen
~/git/stroom-analytic-demo/demonstrator/bash/sendToStroom.sh
```

Where `~/git` should be replaced with the local directory location where you cloned this repo.

Alternatively, it is possible to manually upload the files created by `EventGen` to the appropriate Stroom feeds via the
Stroom UI.

## Stroom
The modified version of Stroom is currently located on the  analytic-demo` branch.

### Kafka
The modified version of Stroom provides the pipeline filter element `StandardKafkaProducer`.  This expects to receive
events in the form of `kafka-records` XML documents.
These records are placed onto Kafka using the producer properties supplied to the filter.

Every `kafka-record` can specify:
* The kafka **topic** to be targeted
* The kafka **partition** that will be used
* The message **key**
* The message **timestamp** that will be used
* Any kafka **headers** that may be required
* The message **value** (i.e. the message itself)

### Annotations
The modified version of Stroom provides the pipeline filter element `AnnotationWriter`.  This expects to receive
annotations in the form of `annotation-records` XML documents.


### Authentication and Authorisation
The standard `local.yml` as generated by `local.yml.sh` can be used for local test enviroments. However, it is 
necessary to create an API Key for a user with the correct permissions. Note: `admin` is suitable, but for obvious reasons,
should only be used in standalone/test environments.  

The API Key should be assigned to the environmental variable `STROOM_API_KEY` prior to starting any of the scripts or 
the Jupyter notebook server.

Alternatively, in standalone/test environments only, it is possible to disable authentication for Stroom by changing the
`local.yml` by changing the `authenticationRequired` property to `false`.
```
    authenticationRequired: false
```
 

## Stroom content
This repo provides Stroom content in the directory `demonstrator/stroom`.  Either of the following Stroom archives
should be imported into a running Stroom instance:
`StroomConfig.zip` contains all the content that is special to this demonstrator.
`StroomConfig-all.zip` as above but also includes dependencies.

The following content is included:
1. **kafka-records** *Stroom XML schema definition* that defines the format required by `StandardKafkaProducer`
1. **detection** *Stroom XML schema definition* that defines a special type of event suitable for representing analytic results.
1. **annotation** *Stroom XML schema definition* that defines a special type of event that can be used to create annotations.
1. **Analytic Demonstrator** - a folder containing a variety of content including feeds, pipelines, XSLT, dashboards and indexes.

### Enabling Stroom Content
In order to enable the imported Stroom content, it is necessary to create processor filters and volume groups via the Stroom UI.

#### Create Volume Groups.
1. Create a Index Volume Group called `Group1` containing a single volume with a Node name of `node1a` and a path of
`$HOME/stroom/analyticdemo/indexvols/group1`
1. Open the index `System/Analytic Demonstrator/Sample Index/Sample Index` using the Stroom UI. 
Ensure that the Volume Group `Group1` is selected
1. Open the index `System/Analytic Demonstrator/Analytic Output/Detections/Index/Detections Index` using the Stroom UI. 
Ensure that the Volume Group `Group1` is selected

#### Create Processor Filters
Using the Stroom UI, processor filters should be created on the following pipelines.  This should be done manually in the Stroom UI.
For example, 1 below can be achived by creating a new processor on the pipeline`DEMO-EVENTS` 
with the filter set to `Type = 'Raw Events' AND (Feed Name = 'DEMO-MAINFRAME-EVENTS' OR Feed Name = 'DEMO-VPN-EVENTS')`:
1. **DEMO-EVENTS** to convert all `Raw Events` streams on feeds `DEMO-MAINFRAME-EVENTS` and `DEMO-VPN-EVENTS` into `event-logging` XML.
1. **Sample Index** to place all `Events` streams on feeds `DEMO-MAINFRAME-EVENTS` and `DEMO-VPN-EVENTS` into the index.
1. **Sample Topic** to place all `Events` streams on feeds `DEMO-MAINFRAME-EVENTS` and `DEMO-VPN-EVENTS` onto the topic.
1. **Detect Unusual Login Times** to run this single event / simple analysis against all `Events` streams on feeds `DEMO-MAINFRAME-EVENTS` and `DEMO-VPN-EVENTS`
1. **SAMPLE-DETECTIONS** to convert all `Raw Events` streams on feed `SAMPLE-DETECTIONS` into `detection` XML (type is `Detections`)
1. **Detections Index** to place all `Detections` streams on any feed into the index.
1. **SAMPLE-ALERTS** to create Annotations from all `Raw Events` streams on feed `SAMPLE-ALERTS`.

#### Enable Stream Processing
Enable Stream processing from the `Monitoring/Jobs` dialog of the Stroom UI (both globally and on `node1a`, assuming that this is a default, local installation)

## Next Steps
It is now possible to proceed with static and streaming data [analysis](analysis.md).
