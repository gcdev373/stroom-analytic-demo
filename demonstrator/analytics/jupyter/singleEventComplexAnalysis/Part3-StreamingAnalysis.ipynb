{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Event with Complex Analysis\n",
    "\n",
    "## Part Three (Streaming Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "This notebook is designed to work with a Stroom server process running on `localhost`, into which data from `EventGen` application has been ingested and indexed in the manner described in `stroom-analytic-demo`.\n",
    "\n",
    "You must set the environmental variable `STROOM_API_KEY` to the API token associated with a suitably privileged Stroom user account before starting the Jupyter notebook server process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col, coalesce, unix_timestamp,lit,to_timestamp,hour,date_format,date_trunc,window\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator,VectorAssembler,StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from IPython.display import display\n",
    "import time,os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Discovery\n",
    "It is necessary to specify the structure of the JSON data arriving on the topic.  This structure can be determined at runtime.\n",
    "\n",
    "As the same format of data is also available via an indexed search using the `stroom-spark-datasource`, one way to determine the JSON schema is by interrogating the data held in the `Sample Index` Stroom index.\n",
    "\n",
    "The specified pipeline is a Stroom Search Extraction Pipeline that uses the stroom:json XSLT function to create a JSON representation of the entire event.  This field is called \"Json\" by default but the name of the field that contains the JSON representation can (optionally) be changed with the parameter jsonField.\n",
    "\n",
    "In this manner, all data is returned as a single JSON structure within the field **json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  2851  records for schema discovery.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(EventDetail,StructType(List(StructField(Authenticate,StructType(List(StructField(Action,StringType,true),StructField(Outcome,StructType(List(StructField(Permitted,StringType,true),StructField(Reason,StringType,true),StructField(Success,StringType,true))),true),StructField(User,StructType(List(StructField(Id,StringType,true))),true))),true),StructField(Process,StructType(List(StructField(Action,StringType,true),StructField(Command,StringType,true),StructField(Type,StringType,true))),true),StructField(TypeId,StringType,true))),true),StructField(EventId,StringType,true),StructField(EventSource,StructType(List(StructField(Device,StructType(List(StructField(HostName,StringType,true))),true),StructField(Generator,StringType,true),StructField(System,StructType(List(StructField(Environment,StringType,true),StructField(Name,StringType,true))),true),StructField(User,StructType(List(StructField(Id,StringType,true))),true))),true),StructField(EventTime,StructType(List(StructField(TimeCreated,StringType,true))),true),StructField(StreamId,StringType,true)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MyTestApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schemaDf = spark.read.format('stroom.spark.datasource.StroomDataSource').load(\n",
    "        token=os.environ['STROOM_API_KEY'],host='localhost',protocol='http',\n",
    "        uri='api/stroom-index/v2',traceLevel=\"0\",\n",
    "        index='32dfd401-ee11-49b9-84c9-88c3d3f68dc2',\n",
    "        pipeline='13143179-b494-4146-ac4b-9a6010cada89',\n",
    "        maxResults='50000').filter((col('idxEventTime') > '2018-01-01T00:00:00.000Z')\n",
    "            & (col('idxEventTime') < '2018-01-02T00:00:00.000Z'))\n",
    "\n",
    "print ('Using ', schemaDf.count(), ' records for schema discovery.')\n",
    "json_schema = spark.read.json(schemaDf.rdd.map(lambda row: row.json)).schema\n",
    "\n",
    "json_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframe for streaming analysis\n",
    "Create a Spark DF driven via the Kafka topic\n",
    "\n",
    "### Kafka Headers\n",
    "These are not supported until Spark v3.0, however previous versions do not baulk when the necessary option is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includeHeaders isn't supported until Spark Version > 3.0\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"ANALYTIC-DEMO-UEBA\") \\\n",
    "    .option(\"startingoffsets\", \"latest\")\\\n",
    "    .option(\"includeHeaders\", \"true\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for each batch of data\n",
    "Although Spark will automatically create aggregations (in this case the hourly counts), it is still necessary to define a function that assesses these and if necessary creates an alert to send to Stroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, epoch_id):\n",
    "    df.filter((col('count') > col('prediction') * 2) & \n",
    "             (col('count') > 10)).show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Analysis\n",
    "This analytic is based on code used within the second notebook of this series (Part2-MLTraining).  Hourly counts can be created using Spark Structured Streaming aggregations over time windows. \n",
    "\n",
    "### Two ways to work with windows\n",
    "It is possible to create the windowed counts using window functions using the dataframe function `over` or aggregate functions (grouped by timestamp).\n",
    "\n",
    "The aggregate function approach follows.\n",
    "\n",
    "The window definition used is `window (\"timestamp\", \"1 hour\")`, but it is possible to use sliding windows, e.g. using `window(\"timestamp\", \"1 hour\", \"10 minutes\")` will generate 6 sliding (overlapping) windows in each hour period.  This could allow an analytic to report more rapidly (i.e. not having to wait until an hour has elapsed), but it makes multiple alerts for the same data more likely, and therefore some deduplication of alerts would need to be performed downstream.\n",
    "\n",
    "### Kafka Headers\n",
    "Although Spark v3.0 supports Kafka Headers, there is currently no documentation relating to this topic.  It appears that a new column `headers` is available, which can be cast to `MapType(String,String)`, and then accessed as per any Python dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-------------+-----------+----------+\n",
      "|window|features|count|rawPrediction|probability|prediction|\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "|window|features|count|rawPrediction|probability|prediction|\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "|window|features|count|rawPrediction|probability|prediction|\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "|window|features|count|rawPrediction|probability|prediction|\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "|window|features|count|rawPrediction|probability|prediction|\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|              window|            features|count|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[2019-11-08 18:00...|(29,[18,24],[1.0,...|  249|[-4.0510469612551...|[7.26511294456510...|      10.0|\n",
      "|[2019-11-15 09:00...|(29,[9,24],[1.0,1...|   21|[-4.0461295079745...|[5.90982082497940...|       9.0|\n",
      "|[2019-11-08 17:00...|(29,[17,24],[1.0,...|  146|[-4.0609478070280...|[9.76373295288543...|      15.0|\n",
      "|[2019-11-07 08:00...|(29,[8,26],[1.0,1...|   11|[-4.0324393755160...|[2.65260302368554...|       1.0|\n",
      "|[2019-11-15 08:00...|(29,[8,24],[1.0,1...|   12|[-4.0295600451784...|[2.49436360997083...|       2.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "|window|features|count|rawPrediction|probability|prediction|\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "|window|features|count|rawPrediction|probability|prediction|\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "+------+--------+-----+-------------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2f497e8faa64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#col('headers') should be available on the DataFrame (Spark > 3.0)\n",
    "#Code will be something similar to the line below to be added to the DataFrame definition\n",
    "#withColumn('headerMap', col('headers').cast('MapType(String,String)')).\\\n",
    "wideDf = df.withColumn('json',col('value').cast('string')).\\\n",
    "    withColumn('evt', from_json(col('json'), json_schema)).\\\n",
    "    withColumn ('timestamp', to_timestamp(col('evt.EventTime.TimeCreated')).cast(\"timestamp\")).\\\n",
    "    withColumn('operation', col('evt.EventDetail.TypeId')).\\\n",
    "    filter(col('operation') == 'Authentication Failure' ).\\\n",
    "    withColumn('streamid', col('evt.StreamId')).\\\n",
    "    withColumn('eventid', col('evt.EventId')).\\\n",
    "    dropDuplicates([\"eventid\", \"streamid\"]).\\\n",
    "    groupBy(window (\"timestamp\", \"1 hour\"),\n",
    "            date_format('timestamp', 'EEEE').alias(\"day\"), \n",
    "            hour(\"timestamp\").alias(\"hour\"),).count()\n",
    "\n",
    "\n",
    "\n",
    "pipelineModel = PipelineModel.load(\"models/inputVecPipelineModel\")\n",
    "\n",
    "featuresDf = pipelineModel.transform(wideDf)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['hourVec','dayVec'], outputCol = 'features')\n",
    "\n",
    "fullDf = vectorAssembler.transform(featuresDf).select('window','features','count')\n",
    "\n",
    "lrModel = LogisticRegressionModel.load(\"models/logisticRegressionAuthFailuresModel\")\n",
    "\n",
    "lrDf = lrModel.transform (fullDf)\n",
    "\n",
    "#outputMode can be append, complete or update\n",
    "query = lrDf.writeStream.\\\n",
    "    outputMode(\"update\").\\\n",
    "    foreachBatch (process_batch).\\\n",
    "    start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
