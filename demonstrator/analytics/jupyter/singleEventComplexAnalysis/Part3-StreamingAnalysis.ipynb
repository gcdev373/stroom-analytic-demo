{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Event with Complex Analysis\n",
    "\n",
    "## Part Three (Streaming Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "This notebook is designed to work with a Stroom server process running on `localhost`, into which data from `EventGen` application has been ingested and indexed in the manner described in `stroom-analytic-demo`.\n",
    "\n",
    "You must set the environmental variable `STROOM_API_KEY` to the API token associated with a suitably privileged Stroom user account before starting the Jupyter notebook server process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col, coalesce, unix_timestamp,lit,to_timestamp,hour,date_format,date_trunc,window\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator,VectorAssembler,StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from IPython.display import display\n",
    "import time,os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Discovery\n",
    "It is necessary to specify the structure of the JSON data arriving on the topic.  This structure can be determined at runtime.\n",
    "\n",
    "As the same format of data is also available via an indexed search using the `stroom-spark-datasource`, one way to determine the JSON schema is by interrogating the data held in the `Sample Index` Stroom index.\n",
    "\n",
    "The specified pipeline is a Stroom Search Extraction Pipeline that uses the stroom:json XSLT function to create a JSON representation of the entire event.  This field is called \"Json\" by default but the name of the field that contains the JSON representation can (optionally) be changed with the parameter jsonField.\n",
    "\n",
    "In this manner, all data is returned as a single JSON structure within the field **json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  3465  records for schema discovery.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(EventDetail,StructType(List(StructField(Authenticate,StructType(List(StructField(Action,StringType,true),StructField(Outcome,StructType(List(StructField(Permitted,StringType,true),StructField(Reason,StringType,true),StructField(Success,StringType,true))),true),StructField(User,StructType(List(StructField(Id,StringType,true))),true))),true),StructField(Process,StructType(List(StructField(Action,StringType,true),StructField(Command,StringType,true),StructField(Type,StringType,true))),true),StructField(TypeId,StringType,true))),true),StructField(EventId,StringType,true),StructField(EventSource,StructType(List(StructField(Client,StructType(List(StructField(HostName,StringType,true))),true),StructField(Device,StructType(List(StructField(HostName,StringType,true))),true),StructField(Generator,StringType,true),StructField(System,StructType(List(StructField(Environment,StringType,true),StructField(Name,StringType,true))),true),StructField(User,StructType(List(StructField(Id,StringType,true))),true))),true),StructField(EventTime,StructType(List(StructField(TimeCreated,StringType,true))),true),StructField(StreamId,StringType,true)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MyTestApp\") \\\n",
    "    .getOrCreate()\n",
    "schemaDf = spark.read.format('stroom.spark.datasource.StroomDataSource').load(\n",
    "        token=os.environ['STROOM_API_KEY'],host='localhost',protocol='http',\n",
    "        uri='api/stroom-index/v2',traceLevel=\"0\",\n",
    "        index='32dfd401-ee11-49b9-84c9-88c3d3f68dc2',pipeline='13143179-b494-4146-ac4b-9a6010cada89',\n",
    "        maxResults='300000').filter((col('idxEventTime') > '2018-01-01T00:00:00.000Z')\n",
    "            & (col('idxEventTime') < '2018-01-02T00:00:00.000Z'))\n",
    "\n",
    "print ('Using ', schemaDf.count(), ' records for schema discovery.')\n",
    "json_schema = spark.read.json(schemaDf.rdd.map(lambda row: row.json)).schema\n",
    "\n",
    "json_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframe for streaming analysis\n",
    "Create a Spark DF driven via the Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"ANALYTIC-DEMO-UEBA\") \\\n",
    "  .option(\"startingoffsets\", \"earliest\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways to work with windows\n",
    "It is possible to create the windowed counts using window functions using the dataframe function `over` or aggregate functions (grouped by timestamp).  The first of these calculates a new value for every record, and therefore allows rolling windowed operations to be performed.\n",
    "\n",
    "The aggregate function approach follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, epoch_id):\n",
    "    df.sort(col('count').desc()).show()\n",
    "    #df.filter(col('count') > col('prediction') * 2).show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|              window|            features|count|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[2019-11-05 14:00...|(29,[14,25],[1.0,...|   25|[-4.0702780504231...|[1.45000719330590...|      21.0|\n",
      "|[2019-11-04 13:00...|(29,[13,27],[1.0,...|   24|[-4.0819635222495...|[1.58244653249414...|      18.0|\n",
      "|[2019-11-04 16:00...|(29,[16,27],[1.0,...|   23|[-4.0743370372498...|[1.26650973752681...|      19.0|\n",
      "|[2019-11-05 13:00...|(29,[13,25],[1.0,...|   23|[-4.0739692576681...|[1.58899507436920...|      20.0|\n",
      "|[2019-11-05 11:00...|(29,[11,25],[1.0,...|   22|[-4.0583778204207...|[1.01793374247146...|      16.0|\n",
      "|[2019-11-05 12:00...|(29,[12,25],[1.0,...|   22|[-4.0681788075552...|[1.33565068408936...|      19.0|\n",
      "|[2019-11-01 15:00...|(29,[15,24],[1.0,...|   22|[-4.0693609270898...|[1.33156707516116...|      20.0|\n",
      "|[2019-11-05 15:00...|(29,[15,25],[1.0,...|   21|[-4.0714448947688...|[1.48125947010625...|      20.0|\n",
      "|[2019-11-05 10:00...|(29,[10,25],[1.0,...|   21|[-4.0593594160059...|[9.32898890939701...|      12.0|\n",
      "|[2019-11-01 13:00...|(29,[13,24],[1.0,...|   20|[-4.0718852899891...|[1.42574128501634...|      20.0|\n",
      "|[2019-11-01 14:00...|(29,[14,24],[1.0,...|   19|[-4.0681940827441...|[1.29915162291770...|      20.0|\n",
      "|[2019-11-01 12:00...|(29,[12,24],[1.0,...|   19|[-4.0660948398762...|[1.19356179528235...|      20.0|\n",
      "|[2019-11-04 14:00...|(29,[14,27],[1.0,...|   19|[-4.0782723150044...|[1.39467610076262...|      25.0|\n",
      "|[2019-11-01 16:00...|(29,[16,24],[1.0,...|   18|[-4.0642588049895...|[1.14121771050908...|      21.0|\n",
      "|[2019-11-04 11:00...|(29,[11,27],[1.0,...|   18|[-4.0663720850020...|[1.03808857808873...|      16.0|\n",
      "|[2019-11-04 15:00...|(29,[15,27],[1.0,...|   17|[-4.0794391593501...|[1.48461794255266...|      22.0|\n",
      "|[2019-11-05 09:00...|(29,[9,25],[1.0,1...|   15|[-4.0482134756535...|[6.30740763907962...|       8.0|\n",
      "|[2019-11-04 12:00...|(29,[12,27],[1.0,...|   15|[-4.0761730721365...|[1.34248093842052...|      19.0|\n",
      "|[2019-11-04 17:00...|(29,[17,27],[1.0,...|   13|[-4.0710260392883...|[1.15161184770848...|      15.0|\n",
      "|[2019-11-01 18:00...|(29,[18,24],[1.0,...|   13|[-4.0510469612551...|[7.26511294456510...|      10.0|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#window = Window.orderBy(\"timestamp\").partitionBy()\n",
    "\n",
    "wideDf = df.withColumn('json',col('value').cast('string')).\\\n",
    "    withColumn('evt', from_json(col('json'), json_schema)).\\\n",
    "    withColumn ('timestamp', to_timestamp(col('evt.EventTime.TimeCreated')).cast(\"timestamp\")).\\\n",
    "    withColumn('operation', col('evt.EventDetail.TypeId')).\\\n",
    "    filter(col('operation') == 'Authentication Failure').\\\n",
    "    withColumn('streamid', col('evt.StreamId')).\\\n",
    "    withColumn('eventid', col('evt.EventId')).\\\n",
    "    withColumn('day', date_format('timestamp', 'EEEE')).\\\n",
    "    withColumn('hour', hour(\"timestamp\").alias(\"hour\")).\\\n",
    "    dropDuplicates([\"eventid\", \"streamid\"]).\\\n",
    "    groupBy(window (\"timestamp\", \"1 hour\"),'day','hour','operation').count()\n",
    "\n",
    "pipelineModel = PipelineModel.load(\"models/inputVecPipelineModel\")\n",
    "\n",
    "# lrModel.save(\"lrAuthFailuresModel\")\n",
    "\n",
    "# hourEncoder = OneHotEncoderEstimator(inputCols=['hour'],outputCols=['hourVec'])\n",
    "# dayNameIndexer = StringIndexer(inputCol=\"day\",outputCol=\"dayCat\")\n",
    "# dayEncoder = OneHotEncoderEstimator(inputCols=['dayCat'],outputCols=['dayVec'])\n",
    "# basicPipeline = Pipeline(stages=[hourEncoder, dayNameIndexer, dayEncoder])\n",
    "\n",
    "# vecDf = basicPipeline.fit(wideDf).transform(wideDf)\n",
    "\n",
    "# vectorAssembler = VectorAssembler(inputCols = ['hourVec','dayVec'], outputCol = 'features')\n",
    "\n",
    "# featuresDf = vectorAssembler.transform(vecDf).select('features','count')\n",
    "\n",
    "featuresDf = pipelineModel.transform(wideDf)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['hourVec','dayVec'], outputCol = 'features')\n",
    "\n",
    "fullDf = vectorAssembler.transform(featuresDf).select('window','features','count')\n",
    "\n",
    "lrModel = LogisticRegressionModel.load(\"models/logisticRegressionAuthFailuresModel\")\n",
    "\n",
    "lrDf = lrModel.transform (fullDf)\n",
    "\n",
    "#outputMode can be append, complete or update\n",
    "query = lrDf.writeStream.\\\n",
    "    foreachBatch (process_batch).\\\n",
    "    outputMode(\"complete\").\\\n",
    "    start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the entire feature vector.  Shown below with what will be the required output vector (actually a simple scalar \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
