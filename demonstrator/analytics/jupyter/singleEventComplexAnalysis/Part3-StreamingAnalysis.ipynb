{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Event with Complex Analysis\n",
    "\n",
    "## Part Three (Streaming Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "This notebook is designed to work with a Stroom server process running on `localhost`, into which data from `EventGen` application has been ingested and indexed in the manner described in `stroom-analytic-demo`.\n",
    "\n",
    "You must set the environmental variable `STROOM_API_KEY` to the API token associated with a suitably privileged Stroom user account before starting the Jupyter notebook server process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col, coalesce, unix_timestamp,lit,to_timestamp,hour,date_format,date_trunc\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator,VectorAssembler,StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from IPython.display import display\n",
    "import time,os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Discovery\n",
    "It is necessary to specify the structure of the JSON data arriving on the topic.  This structure can be determined at runtime.\n",
    "\n",
    "As the same format of data is also available via an indexed search using the `stroom-spark-datasource`, one way to determine the JSON schema is by interrogating the data held in the `Sample Index` Stroom index.\n",
    "\n",
    "The specified pipeline is a Stroom Search Extraction Pipeline that uses the stroom:json XSLT function to create a JSON representation of the entire event.  This field is called \"Json\" by default but the name of the field that contains the JSON representation can (optionally) be changed with the parameter jsonField.\n",
    "\n",
    "In this manner, all data is returned as a single JSON structure within the field **json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  56889  records for schema discovery.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(EventDetail,StructType(List(StructField(Authenticate,StructType(List(StructField(Action,StringType,true),StructField(Outcome,StructType(List(StructField(Permitted,StringType,true),StructField(Reason,StringType,true),StructField(Success,StringType,true))),true),StructField(User,StructType(List(StructField(Id,StringType,true))),true))),true),StructField(TypeId,StringType,true))),true),StructField(EventId,StringType,true),StructField(EventSource,StructType(List(StructField(Device,StructType(List(StructField(HostName,StringType,true))),true),StructField(Generator,StringType,true),StructField(System,StructType(List(StructField(Environment,StringType,true),StructField(Name,StringType,true))),true))),true),StructField(EventTime,StructType(List(StructField(TimeCreated,StringType,true))),true),StructField(StreamId,StringType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MyTestApp\") \\\n",
    "    .getOrCreate()\n",
    "schemaDf = spark.read.format('stroom.spark.datasource.StroomDataSource').load(\n",
    "        token=os.environ['STROOM_API_KEY'],host='localhost',protocol='http',\n",
    "        uri='api/stroom-index/v2',traceLevel=\"0\",\n",
    "        index='32dfd401-ee11-49b9-84c9-88c3d3f68dc2',pipeline='13143179-b494-4146-ac4b-9a6010cada89',\n",
    "        maxResults='300000').filter((col('idxEventTime') > '2018-01-01T00:00:00.000Z')\n",
    "            & (col('idxEventTime') < '2019-01-01T00:00:00.000Z')\n",
    "            & (col('idxDescription') == 'Authentication Failure'))\n",
    "\n",
    "print ('Using ', schemaDf.count(), ' records for schema discovery.')\n",
    "json_schema = spark.read.json(schemaDf.rdd.map(lambda row: row.json)).schema\n",
    "\n",
    "json_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Creating a feature vector suitable for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDf = schemaDf.withColumn('evt', from_json(col('json'), json_schema)).\\\n",
    "    withColumn ('timestamp', to_timestamp(col('evt.EventTime.TimeCreated')).cast(\"timestamp\")).\\\n",
    "    where(col(\"timestamp\") < '2019-01-01').\\\n",
    "    withColumn('operation', col('evt.EventDetail.TypeId')).\\\n",
    "    groupBy(date_trunc('day',\"timestamp\").alias(\"date\"), \n",
    "            date_format('timestamp', 'EEEE').alias(\"day\"), \n",
    "            hour(\"timestamp\").alias(\"hour\"),\n",
    "            col('operation')).\\\n",
    "    count().sort(col('date'),col('hour'))\n",
    "\n",
    "hourEncoder = OneHotEncoderEstimator(inputCols=['hour'],outputCols=['hourVec'])\n",
    "dayNameIndexer = StringIndexer(inputCol=\"day\",outputCol=\"dayCat\")\n",
    "dayEncoder = OneHotEncoderEstimator(inputCols=['dayCat'],outputCols=['dayVec'])\n",
    "basicPipeline = Pipeline(stages=[hourEncoder, dayNameIndexer, dayEncoder])\n",
    "\n",
    "vecDf = basicPipeline.fit(featuresDf).transform(featuresDf)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['hourVec','dayVec'], outputCol = 'features')\n",
    "\n",
    "trainingDf = vectorAssembler.transform(vecDf).select('features','count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the entire feature vector.  Shown below with what will be the required output vector (actually a simple scalar \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|count|\n",
      "+--------------------+-----+\n",
      "|(29,[12,27],[1.0,...|   11|\n",
      "|(29,[13,27],[1.0,...|   33|\n",
      "|(29,[14,27],[1.0,...|   15|\n",
      "|(29,[15,27],[1.0,...|   24|\n",
      "|(29,[16,27],[1.0,...|   11|\n",
      "|(29,[17,27],[1.0,...|   10|\n",
      "|(29,[18,27],[1.0,...|   14|\n",
      "|(29,[19,27],[1.0,...|   11|\n",
      "|(29,[20,27],[1.0,...|    5|\n",
      "|(29,[21,27],[1.0,...|    2|\n",
      "|(29,[22,27],[1.0,...|    2|\n",
      "|     (29,[27],[1.0])|    3|\n",
      "|(29,[0,25],[1.0,1...|    2|\n",
      "|(29,[1,25],[1.0,1...|    1|\n",
      "|(29,[2,25],[1.0,1...|    2|\n",
      "|(29,[3,25],[1.0,1...|    3|\n",
      "|(29,[4,25],[1.0,1...|    1|\n",
      "|(29,[5,25],[1.0,1...|    1|\n",
      "|(29,[7,25],[1.0,1...|    2|\n",
      "|(29,[8,25],[1.0,1...|    8|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['hourVec','dayVec'], outputCol = 'features')\n",
    "\n",
    "trainingDf = vectorAssembler.transform(vecDf).select('features','count')\n",
    "\n",
    "trainingDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now create a Logistic Regression classifier to predict the number of auth failures in each hour/day of week.\n",
    "Save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.001, featuresCol='features', labelCol='count')\n",
    "\n",
    "lrModel = lr.fit(trainingDf)\n",
    "\n",
    "lrModel.save(\"lrAuthFailuresModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "There are many ways that an ML model could be refined and improved.  Here we are only interested in understanding whether the model fits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|count|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(29,[12,27],[1.0,...|   11|[-2.7456508843618...|[6.04109790047702...|      19.0|\n",
      "|(29,[13,27],[1.0,...|   33|[-2.7481406864495...|[5.20069772206273...|      27.0|\n",
      "|(29,[14,27],[1.0,...|   15|[-2.7512653300797...|[6.22760737886773...|      27.0|\n",
      "|(29,[15,27],[1.0,...|   24|[-2.7381587292718...|[5.24014825680056...|      19.0|\n",
      "|(29,[16,27],[1.0,...|   11|[-2.7290683973745...|[3.73174281264897...|      19.0|\n",
      "|(29,[17,27],[1.0,...|   10|[-2.7359188437171...|[4.79819153343026...|      10.0|\n",
      "|(29,[18,27],[1.0,...|   14|[-2.7159320447787...|[2.12571076937581...|      11.0|\n",
      "|(29,[19,27],[1.0,...|   11|[-2.7141243285646...|[1.72485237287030...|       7.0|\n",
      "|(29,[20,27],[1.0,...|    5|[-2.7010271295796...|[1.26676494165678...|       6.0|\n",
      "|(29,[21,27],[1.0,...|    2|[-2.6939930777854...|[9.57053660352971...|       5.0|\n",
      "|(29,[22,27],[1.0,...|    2|[-2.6827699576494...|[6.24407483109714...|       1.0|\n",
      "|     (29,[27],[1.0])|    3|[-2.6332143399696...|[6.65521142898168...|       1.0|\n",
      "|(29,[0,25],[1.0,1...|    2|[-2.6587217290297...|[2.68250571981964...|       1.0|\n",
      "|(29,[1,25],[1.0,1...|    1|[-2.6463099136409...|[2.40743493605442...|       2.0|\n",
      "|(29,[2,25],[1.0,1...|    2|[-2.6505063628672...|[1.92736823601741...|       1.0|\n",
      "|(29,[3,25],[1.0,1...|    3|[-2.6488348502790...|[1.85795640663387...|       1.0|\n",
      "|(29,[4,25],[1.0,1...|    1|[-2.6446031317669...|[1.06947738461252...|       1.0|\n",
      "|(29,[5,25],[1.0,1...|    1|[-2.6364971157348...|[5.7759820507792E...|       1.0|\n",
      "|(29,[7,25],[1.0,1...|    2|[-2.6620696257593...|[3.54756612753944...|       2.0|\n",
      "|(29,[8,25],[1.0,1...|    8|[-2.6856917324549...|[1.29809374252811...|       6.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrModel.transform(trainingDf).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- json: string (nullable = true)\n",
      " |-- evt: struct (nullable = true)\n",
      " |    |-- EventDetail: struct (nullable = true)\n",
      " |    |    |-- Authenticate: struct (nullable = true)\n",
      " |    |    |    |-- Action: string (nullable = true)\n",
      " |    |    |    |-- Outcome: struct (nullable = true)\n",
      " |    |    |    |    |-- Permitted: string (nullable = true)\n",
      " |    |    |    |    |-- Reason: string (nullable = true)\n",
      " |    |    |    |    |-- Success: string (nullable = true)\n",
      " |    |    |    |-- User: struct (nullable = true)\n",
      " |    |    |    |    |-- Id: string (nullable = true)\n",
      " |    |    |-- Process: struct (nullable = true)\n",
      " |    |    |    |-- Action: string (nullable = true)\n",
      " |    |    |    |-- Command: string (nullable = true)\n",
      " |    |    |    |-- Type: string (nullable = true)\n",
      " |    |    |-- TypeId: string (nullable = true)\n",
      " |    |-- EventId: string (nullable = true)\n",
      " |    |-- EventSource: struct (nullable = true)\n",
      " |    |    |-- Client: struct (nullable = true)\n",
      " |    |    |    |-- HostName: string (nullable = true)\n",
      " |    |    |-- Device: struct (nullable = true)\n",
      " |    |    |    |-- HostName: string (nullable = true)\n",
      " |    |    |-- Generator: string (nullable = true)\n",
      " |    |    |-- System: struct (nullable = true)\n",
      " |    |    |    |-- Environment: string (nullable = true)\n",
      " |    |    |    |-- Name: string (nullable = true)\n",
      " |    |    |-- User: struct (nullable = true)\n",
      " |    |    |    |-- Id: string (nullable = true)\n",
      " |    |-- EventTime: struct (nullable = true)\n",
      " |    |    |-- TimeCreated: string (nullable = true)\n",
      " |    |-- StreamId: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- streamid: string (nullable = true)\n",
      " |-- eventid: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f907c8100ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spark/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"ANALYTIC-DEMO-UEBA\") \\\n",
    "  .option(\"startingoffsets\", \"earliest\")\\\n",
    "  .load()\n",
    "\n",
    "wideDf = df.withColumn('json',col('value').cast('string')).\\\n",
    "    withColumn('evt', from_json(col('json'), json_schema)).\\\n",
    "    withColumn ('timestamp', to_timestamp(col('evt.EventTime.TimeCreated')).cast(\"timestamp\")).\\\n",
    "    withColumn('operation', col('evt.EventDetail.TypeId')).\\\n",
    "    withColumn('streamid', col('evt.StreamId')).\\\n",
    "    withColumn('eventid', col('evt.EventId')).\\\n",
    "    withColumn ('user', coalesce(col('evt.EventSource.User.Id'),\\\n",
    "                                                col('evt.EventDetail.Authenticate.User.Id'))).\\\n",
    "    withWatermark(\"timestamp\", \"30 minutes\").\\\n",
    "    dropDuplicates([\"eventid\", \"streamid\"])\n",
    "\n",
    "\n",
    "wideDf.printSchema()\n",
    "\n",
    "wideDf.createOrReplaceTempView(\"userops\")\n",
    "\n",
    "#userCounts=spark.sql(\"select user from userops\")\n",
    "\n",
    "userCounts = spark.sql(\"select user,operation, count (streamid, eventid) as events from userops \\\n",
    "                    group by user,operation\")\n",
    "\n",
    "#N.B. You cannot order results of the above, unless using outputMode(\"complete\")\n",
    "               \n",
    "query = userCounts \\\n",
    " .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
