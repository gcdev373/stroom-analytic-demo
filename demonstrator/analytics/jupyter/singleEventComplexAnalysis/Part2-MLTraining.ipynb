{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Event with Complex Analysis\n",
    "\n",
    "## Part Two (ML Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "This notebook is designed to work with a Stroom server process running on `localhost`, into which data from `EventGen` application has been ingested and indexed in the manner described in `stroom-analytic-demo`.\n",
    "\n",
    "You must set the environmental variable `STROOM_API_KEY` to the API token associated with a suitably privileged Stroom user account before starting the Jupyter notebook server process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col, coalesce, unix_timestamp,lit,to_timestamp,hour,date_format,date_trunc\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator,VectorAssembler,StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression,RandomForestRegressor\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from IPython.display import display\n",
    "import time,os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Discovery\n",
    "It is necessary to specify the structure of the JSON data arriving on the topic.  This structure can be determined at runtime.\n",
    "\n",
    "As the same format of data is also available via an indexed search using the `stroom-spark-datasource`, one way to determine the JSON schema is by interrogating the data held in the `Sample Index` Stroom index.\n",
    "\n",
    "The specified pipeline is a Stroom Search Extraction Pipeline that uses the stroom:json XSLT function to create a JSON representation of the entire event.  This field is called \"Json\" by default but the name of the field that contains the JSON representation can (optionally) be changed with the parameter jsonField.\n",
    "\n",
    "In this manner, all data is returned as a single JSON structure within the field **json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  56889  records for training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(EventDetail,StructType(List(StructField(Authenticate,StructType(List(StructField(Action,StringType,true),StructField(Outcome,StructType(List(StructField(Permitted,StringType,true),StructField(Reason,StringType,true),StructField(Success,StringType,true))),true),StructField(User,StructType(List(StructField(Id,StringType,true))),true))),true),StructField(TypeId,StringType,true))),true),StructField(EventId,StringType,true),StructField(EventSource,StructType(List(StructField(Device,StructType(List(StructField(HostName,StringType,true))),true),StructField(Generator,StringType,true),StructField(System,StructType(List(StructField(Environment,StringType,true),StructField(Name,StringType,true))),true))),true),StructField(EventTime,StructType(List(StructField(TimeCreated,StringType,true))),true),StructField(StreamId,StringType,true)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MyTestApp\") \\\n",
    "    .getOrCreate()\n",
    "schemaDf = spark.read.format('stroom.spark.datasource.StroomDataSource').load(\n",
    "        token=os.environ['STROOM_API_KEY'],host='localhost',protocol='http',\n",
    "        uri='api/stroom-index/v2',traceLevel=\"0\",\n",
    "        index='32dfd401-ee11-49b9-84c9-88c3d3f68dc2',pipeline='13143179-b494-4146-ac4b-9a6010cada89',\n",
    "        maxResults='300000').filter((col('idxEventTime') > '2018-01-01T00:00:00.000Z')\n",
    "            & (col('idxEventTime') < '2019-01-01T00:00:00.000Z')\n",
    "            & (col('idxDescription') == 'Authentication Failure'))\n",
    "\n",
    "print ('Using ', schemaDf.count(), ' records for training')\n",
    "json_schema = spark.read.json(schemaDf.rdd.map(lambda row: row.json)).schema\n",
    "\n",
    "json_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Creating a feature vector suitable for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----+--------------------+-----+---------------+------+-------------+\n",
      "|               date|    day|hour|           operation|count|        hourVec|dayCat|       dayVec|\n",
      "+-------------------+-------+----+--------------------+-----+---------------+------+-------------+\n",
      "|2018-01-01 00:00:00| Monday|  12|Authentication Fa...|   11|(23,[12],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  13|Authentication Fa...|   33|(23,[13],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  14|Authentication Fa...|   15|(23,[14],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  15|Authentication Fa...|   24|(23,[15],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  16|Authentication Fa...|   11|(23,[16],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  17|Authentication Fa...|   10|(23,[17],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  18|Authentication Fa...|   14|(23,[18],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  19|Authentication Fa...|   11|(23,[19],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  20|Authentication Fa...|    5|(23,[20],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  21|Authentication Fa...|    2|(23,[21],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  22|Authentication Fa...|    2|(23,[22],[1.0])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-01 00:00:00| Monday|  23|Authentication Fa...|    3|     (23,[],[])|   4.0|(6,[4],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   0|Authentication Fa...|    2| (23,[0],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   1|Authentication Fa...|    1| (23,[1],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   2|Authentication Fa...|    2| (23,[2],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   3|Authentication Fa...|    3| (23,[3],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   4|Authentication Fa...|    1| (23,[4],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   5|Authentication Fa...|    1| (23,[5],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   7|Authentication Fa...|    2| (23,[7],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "|2018-01-02 00:00:00|Tuesday|   8|Authentication Fa...|    8| (23,[8],[1.0])|   2.0|(6,[2],[1.0])|\n",
      "+-------------------+-------+----+--------------------+-----+---------------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresDf = schemaDf.withColumn('evt', from_json(col('json'), json_schema)).\\\n",
    "    withColumn ('timestamp', to_timestamp(col('evt.EventTime.TimeCreated')).cast(\"timestamp\")).\\\n",
    "    where(col(\"timestamp\") < '2019-01-01').\\\n",
    "    withColumn('operation', col('evt.EventDetail.TypeId')).\\\n",
    "    groupBy(date_trunc('day',\"timestamp\").alias(\"date\"), \n",
    "            date_format('timestamp', 'EEEE').alias(\"day\"), \n",
    "            hour(\"timestamp\").alias(\"hour\"),\n",
    "            col('operation')).\\\n",
    "    count().sort(col('date'),col('hour'))\n",
    "\n",
    "\n",
    "# operationNameIndexer = StringIndexer(inputCol=\"operation\",outputCol=\"opCat\")\n",
    "# operationEncoder = OneHotEncoderEstimator(inputCols=['opCat'],outputCols=['opVec'])\n",
    "hourEncoder = OneHotEncoderEstimator(inputCols=['hour'],outputCols=['hourVec'])\n",
    "dayNameIndexer = StringIndexer(inputCol=\"day\",outputCol=\"dayCat\")\n",
    "dayEncoder = OneHotEncoderEstimator(inputCols=['dayCat'],outputCols=['dayVec'])\n",
    "basicPipeline = Pipeline(stages=[hourEncoder, dayNameIndexer, dayEncoder])\n",
    "\n",
    "pipelineModel = basicPipeline.fit(featuresDf)\n",
    "pipelineModel.write().overwrite().save(\"models/inputVecPipelineModel\")\n",
    "\n",
    "vecDf = pipelineModel.transform(featuresDf)\n",
    "\n",
    "vecDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the entire feature vector.  Shown below with what will be the required output vector (actually a simple scalar \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|count|\n",
      "+--------------------+-----+\n",
      "|(29,[12,27],[1.0,...|   11|\n",
      "|(29,[13,27],[1.0,...|   33|\n",
      "|(29,[14,27],[1.0,...|   15|\n",
      "|(29,[15,27],[1.0,...|   24|\n",
      "|(29,[16,27],[1.0,...|   11|\n",
      "|(29,[17,27],[1.0,...|   10|\n",
      "|(29,[18,27],[1.0,...|   14|\n",
      "|(29,[19,27],[1.0,...|   11|\n",
      "|(29,[20,27],[1.0,...|    5|\n",
      "|(29,[21,27],[1.0,...|    2|\n",
      "|(29,[22,27],[1.0,...|    2|\n",
      "|     (29,[27],[1.0])|    3|\n",
      "|(29,[0,25],[1.0,1...|    2|\n",
      "|(29,[1,25],[1.0,1...|    1|\n",
      "|(29,[2,25],[1.0,1...|    2|\n",
      "|(29,[3,25],[1.0,1...|    3|\n",
      "|(29,[4,25],[1.0,1...|    1|\n",
      "|(29,[5,25],[1.0,1...|    1|\n",
      "|(29,[7,25],[1.0,1...|    2|\n",
      "|(29,[8,25],[1.0,1...|    8|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['hourVec','dayVec'], outputCol = 'features')\n",
    "\n",
    "trainingDf = vectorAssembler.transform(vecDf).select('features','count')\n",
    "\n",
    "trainingDf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Linear Regression)\n",
    "Now create a Linear Regression to predict the number of auth failures in each hour/day of week.\n",
    "Save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearReg = LinearRegression(maxIter=20, regParam=0.001, featuresCol='features', labelCol='count')\n",
    "\n",
    "linearRegModel = linearReg.fit(trainingDf)\n",
    "\n",
    "linearRegModel.write().overwrite().save(\"models/linearRegressionAuthFailuresModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation (Linear Regression)\n",
    "There are many ways that an ML model could be refined and improved.  Here we are only interested in understanding whether the model fits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 3.308953783115157 Residuals DataFrame[residuals: double]\n"
     ]
    }
   ],
   "source": [
    "summaryInfo = linearRegModel.evaluate(trainingDf)\n",
    "print (\"Mean Absolute Error\", summaryInfo.meanAbsoluteError, \"Residuals\", summaryInfo.residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|count|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(29,[12,27],[1.0,...|   11|18.263108126586722|\n",
      "|(29,[13,27],[1.0,...|   33|19.685012949148994|\n",
      "|(29,[14,27],[1.0,...|   15|20.270245530353453|\n",
      "|(29,[15,27],[1.0,...|   24|20.304191118372746|\n",
      "|(29,[16,27],[1.0,...|   11| 18.60143897190655|\n",
      "|(29,[17,27],[1.0,...|   10|15.664808076518153|\n",
      "|(29,[18,27],[1.0,...|   14|12.627661091386695|\n",
      "|(29,[19,27],[1.0,...|   11|10.307756618950208|\n",
      "|(29,[20,27],[1.0,...|    5| 8.263558259981334|\n",
      "|(29,[21,27],[1.0,...|    2| 6.535145274602367|\n",
      "|(29,[22,27],[1.0,...|    2| 5.419264963727678|\n",
      "|     (29,[27],[1.0])|    3| 4.169772678038179|\n",
      "|(29,[0,25],[1.0,1...|    2| 4.339590199932597|\n",
      "|(29,[1,25],[1.0,1...|    1|  4.06870403903783|\n",
      "|(29,[2,25],[1.0,1...|    2| 3.855332962540353|\n",
      "|(29,[3,25],[1.0,1...|    3| 3.005186305103371|\n",
      "|(29,[4,25],[1.0,1...|    1|3.4023199990892055|\n",
      "|(29,[5,25],[1.0,1...|    1|2.9789152461253243|\n",
      "|(29,[7,25],[1.0,1...|    2|3.1171856800409357|\n",
      "|(29,[8,25],[1.0,1...|    8| 4.806262770640678|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linearRegModel.transform(trainingDf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Logistic Regression)\n",
    "Although the required prediction is a continuous number, there are possibly so few values that a logistic regression can be used.  Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticReg = LogisticRegression(maxIter=20, regParam=0.001, featuresCol='features', labelCol='count')\n",
    "\n",
    "logisticRegModel = logisticReg.fit(trainingDf)\n",
    "\n",
    "logisticRegModel.write().overwrite().save(\"models/logisticRegressionAuthFailuresModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation (Logistic Regression)\n",
    "There are many ways that an ML model could be refined and improved.  Here we are only interested in understanding whether the model fits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|count|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(29,[12,27],[1.0,...|   11|[-4.0761730721365...|[1.34248093842052...|      19.0|\n",
      "|(29,[13,27],[1.0,...|   33|[-4.0819635222495...|[1.58244653249414...|      18.0|\n",
      "|(29,[14,27],[1.0,...|   15|[-4.0782723150044...|[1.39467610076262...|      25.0|\n",
      "|(29,[15,27],[1.0,...|   24|[-4.0794391593501...|[1.48461794255266...|      22.0|\n",
      "|(29,[16,27],[1.0,...|   11|[-4.0743370372498...|[1.26650973752681...|      19.0|\n",
      "|(29,[17,27],[1.0,...|   10|[-4.0710260392883...|[1.15161184770848...|      15.0|\n",
      "|(29,[18,27],[1.0,...|   14|[-4.0611251935154...|[8.75629418542019...|      11.0|\n",
      "|(29,[19,27],[1.0,...|   11|[-4.0538889215530...|[5.88651530152351...|       9.0|\n",
      "|(29,[20,27],[1.0,...|    5|[-4.0475646017947...|[5.05084667224068...|       7.0|\n",
      "|(29,[21,27],[1.0,...|    2|[-4.0435602143066...|[3.83550378217565...|       5.0|\n",
      "|(29,[22,27],[1.0,...|    2|[-4.0377428429625...|[2.53591249815027...|       4.0|\n",
      "|     (29,[27],[1.0])|    3|[-4.0215585483484...|[1.36162818309217...|       3.0|\n",
      "|(29,[0,25],[1.0,1...|    2|[-4.0229932527822...|[1.29593957342149...|       2.0|\n",
      "|(29,[1,25],[1.0,1...|    1|[-4.0210603148395...|[8.04949659762631...|       1.0|\n",
      "|(29,[2,25],[1.0,1...|    2|[-4.0198012409215...|[5.93974559647028...|       1.0|\n",
      "|(29,[3,25],[1.0,1...|    3|[-4.0181869556112...|[4.44841679311672...|       1.0|\n",
      "|(29,[4,25],[1.0,1...|    1|[-4.0187318630041...|[5.88586387837258...|       1.0|\n",
      "|(29,[5,25],[1.0,1...|    1|[-4.0182620027686...|[4.05099684011964...|       1.0|\n",
      "|(29,[7,25],[1.0,1...|    2|[-4.0206681381008...|[7.65899644259208...|       1.0|\n",
      "|(29,[8,25],[1.0,1...|    8|[-4.0316440128574...|[2.44094682669264...|       2.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logisticRegModel.transform(trainingDf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Random Forest Regression)\n",
    "Maybe a decision tree / random forest approach might be more successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestRegressor = RandomForestRegressor(featuresCol='features', labelCol='count')\n",
    "\n",
    "randomForestModel = randomForestRegressor.fit(trainingDf)\n",
    "\n",
    "randomForestModel.write().overwrite().save(\"models/randomForestAuthFailuresModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation (Random Forest Regression)\n",
    "There are many ways that an ML model could be refined and improved.  Here we are only interested in understanding whether the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|count|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(29,[12,27],[1.0,...|   11|12.091971403530035|\n",
      "|(29,[13,27],[1.0,...|   33|16.773290187280313|\n",
      "|(29,[14,27],[1.0,...|   15|14.901076640562056|\n",
      "|(29,[15,27],[1.0,...|   24| 18.85454716792689|\n",
      "|(29,[16,27],[1.0,...|   11| 14.23039128613128|\n",
      "|(29,[17,27],[1.0,...|   10| 9.372821630417725|\n",
      "|(29,[18,27],[1.0,...|   14| 8.718060315407316|\n",
      "|(29,[19,27],[1.0,...|   11| 8.718060315407316|\n",
      "|(29,[20,27],[1.0,...|    5| 8.718060315407316|\n",
      "|(29,[21,27],[1.0,...|    2| 8.718060315407316|\n",
      "|(29,[22,27],[1.0,...|    2| 8.282301702394571|\n",
      "|     (29,[27],[1.0])|    3| 8.718060315407316|\n",
      "|(29,[0,25],[1.0,1...|    2| 8.077472440375885|\n",
      "|(29,[1,25],[1.0,1...|    1| 6.058537462694453|\n",
      "|(29,[2,25],[1.0,1...|    2| 7.333701272081816|\n",
      "|(29,[3,25],[1.0,1...|    3| 7.791818188774362|\n",
      "|(29,[4,25],[1.0,1...|    1| 8.142222349101697|\n",
      "|(29,[5,25],[1.0,1...|    1| 8.503737500616849|\n",
      "|(29,[7,25],[1.0,1...|    2| 8.503737500616849|\n",
      "|(29,[8,25],[1.0,1...|    8| 8.503737500616849|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForestModel.transform(trainingDf).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
